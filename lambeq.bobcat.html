<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lambeq.bobcat &mdash; lambeq 0.2.4 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> lambeq
            <img src="_static/lambeq_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="parsing.html">Syntactic parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="string_diagrams.html">String diagrams</a></li>
<li class="toctree-l1"><a class="reference internal" href="discopy.html">DisCoPy</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributing to lambeq</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/sentence-input.html">Step 1. Sentence input</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/rewrite.html">Step 2. Diagram rewriting</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/parameterise.html">Step 3. Parameterisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Step 4: Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual_training.html">Advanced: Manual training</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced.html">Advanced: DisCoPy usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/extend-lambeq.html">Advanced: Extending lambeq</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Toolkit</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="root-api.html">lambeq package</a></li>
<li class="toctree-l1"><a class="reference internal" href="package-api.html">Subpackages</a></li>
<li class="toctree-l1"><a class="reference internal" href="uml-diagrams.html">Class diagrams</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-line interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="release_notes.html">Release notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">External links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://qnlp.cambridgequantum.com/downloads.html">Resources</a></li>
<li class="toctree-l1"><a class="reference external" href="https://qnlp.cambridgequantum.com/generate.html">Web demo</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discopy.readthedocs.io">DisCoPy</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">lambeq</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>lambeq.bobcat</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/CQCL/lambeq/blob/main/docs/lambeq.bobcat.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="module-lambeq.bobcat">
<span id="lambeq-bobcat"></span><h1>lambeq.bobcat<a class="headerlink" href="#module-lambeq.bobcat" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">BertForChartClassification</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">lambeq.bobcat.tagger.ChartClassifierConfig</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#BertForChartClassification"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.bert.modeling_bert.BertPreTrainedModel</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.T_destination" title="Permalink to this definition"></a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Call self as a function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">lambeq.bobcat.tagger.ChartClassifierConfig</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#BertForChartClassification.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.add_memory_hooks">
<span class="sig-name descname"><span class="pre">add_memory_hooks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.add_memory_hooks" title="Permalink to this definition"></a></dt>
<dd><p>Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.</p>
<p>Increase in memory consumption is stored in a <cite>mem_rss_diff</cite> attribute for each module and can be reset to zero
with <cite>model.reset_memory_hooks_state()</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.add_module" title="Permalink to this definition"></a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>name (string): name of the child module. The child module can be</dt><dd><p>accessed from this module using the given name</p>
</dd>
</dl>
<p>module (Module): child module to be added to the module.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.adjust_logits_during_generation">
<span class="sig-name descname"><span class="pre">adjust_logits_during_generation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.FloatTensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.FloatTensor</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.adjust_logits_during_generation" title="Permalink to this definition"></a></dt>
<dd><p>Implement in subclasses of [<cite>PreTrainedModel</cite>] for custom behavior to adjust the logits in the generate method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.apply" title="Permalink to this definition"></a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>fn (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None): function to be applied to each submodule</p>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.base_model">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">base_model</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.nn.modules.module.Module</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.base_model" title="Permalink to this definition"></a></dt>
<dd><p><cite>torch.nn.Module</cite>: The main body of the model.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.base_model_prefix">
<span class="sig-name descname"><span class="pre">base_model_prefix</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'bert'</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.base_model_prefix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.beam_sample">
<span class="sig-name descname"><span class="pre">beam_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_scorer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.generation_beam_search.BeamScorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_warper</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_utils.BeamSampleEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSampleDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.beam_sample" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head using <strong>beam search multinomial
sampling</strong> and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</p>
<p>Parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>input_ids (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>):</dt><dd><p>The sequence used as a prompt for the generation.</p>
</dd>
<dt>beam_scorer (<cite>BeamScorer</cite>):</dt><dd><p>A derived instance of [<cite>BeamScorer</cite>] that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of [<cite>BeamScorer</cite>] should be read.</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsProcessor</cite>]
used to modify the prediction scores of the language modeling head applied at each generation step.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>StoppingCriteriaList</cite>]. List of instances of class derived from [<cite>StoppingCriteria</cite>]
used to tell if the generation loop should stop.</p>
</dd>
<dt>logits_warper (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsWarper</cite>] used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to 20):</dt><dd><p><strong>DEPRECATED</strong>. Use <cite>logits_processor</cite> or <cite>stopping_criteria</cite> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific kwargs will be forwarded to the <cite>forward</cite> function of the model. If model is
an encoder-decoder model the kwargs should include <cite>encoder_outputs</cite>.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Return:</dt><dd><p>[<cite>~generation_utils.BeamSampleDecoderOnlyOutput</cite>], [<cite>~generation_utils.BeamSampleEncoderDecoderOutput</cite>] or
<cite>torch.LongTensor</cite>: A <cite>torch.LongTensor</cite> containing the generated tokens (default behaviour) or a
[<cite>~generation_utils.BeamSampleDecoderOnlyOutput</cite>] if <cite>model.config.is_encoder_decoder=False</cite> and
<cite>return_dict_in_generate=True</cite> or a [<cite>~generation_utils.BeamSampleEncoderDecoderOutput</cite>] if
<cite>model.config.is_encoder_decoder=True</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
&gt;&gt;&gt; from transformers import (
…     AutoTokenizer,
…     AutoModelForSeq2SeqLM,
…     LogitsProcessorList,
…     MinLengthLogitsProcessor,
…     TopKLogitsWarper,
…     TemperatureLogitsWarper,
…     BeamSearchScorer,
… )
&gt;&gt;&gt; import torch</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">&quot;translate English to German: How old are you?&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run beam search using 3 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span>
<span class="gp">... </span>        <span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_warper</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">TemperatureLogitsWarper</span><span class="p">(</span><span class="mf">0.7</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">beam_sample</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Wie alt bist du?&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.beam_search">
<span class="sig-name descname"><span class="pre">beam_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_scorer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.generation_beam_search.BeamScorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_utils.BeamSearchEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSearchDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.beam_search" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head using <strong>beam search decoding</strong> and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</p>
<p>Parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>input_ids (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>):</dt><dd><p>The sequence used as a prompt for the generation.</p>
</dd>
<dt>beam_scorer (<cite>BeamScorer</cite>):</dt><dd><p>An derived instance of [<cite>BeamScorer</cite>] that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of [<cite>BeamScorer</cite>] should be read.</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsProcessor</cite>]
used to modify the prediction scores of the language modeling head applied at each generation step.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>StoppingCriteriaList</cite>]. List of instances of class derived from [<cite>StoppingCriteria</cite>]
used to tell if the generation loop should stop.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to 20):</dt><dd><p><strong>DEPRECATED</strong>. Use <cite>logits_processor</cite> or <cite>stopping_criteria</cite> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific kwargs will be forwarded to the <cite>forward</cite> function of the model. If model is
an encoder-decoder model the kwargs should include <cite>encoder_outputs</cite>.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Return:</dt><dd><p>[<cite>generation_utilsBeamSearchDecoderOnlyOutput</cite>], [<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>] or
<cite>torch.LongTensor</cite>: A <cite>torch.LongTensor</cite> containing the generated tokens (default behaviour) or a
[<cite>~generation_utils.BeamSearchDecoderOnlyOutput</cite>] if <cite>model.config.is_encoder_decoder=False</cite> and
<cite>return_dict_in_generate=True</cite> or a [<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>] if
<cite>model.config.is_encoder_decoder=True</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a>python
&gt;&gt;&gt; from transformers import (
…     AutoTokenizer,
…     AutoModelForSeq2SeqLM,
…     LogitsProcessorList,
…     MinLengthLogitsProcessor,
…     BeamSearchScorer,
… )
&gt;&gt;&gt; import torch</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">&quot;translate English to German: How old are you?&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run beam search using 3 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span>
<span class="gp">... </span>        <span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Wie alt bist du?&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.bfloat16" title="Permalink to this definition"></a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.buffers" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>recurse (bool): if True, then yields buffers of this module</dt><dd><p>and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
</dl>
</dd>
<dt>Yields:</dt><dd><p>torch.Tensor: module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.children" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>Module: a child module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.compute_transition_beam_scores">
<span class="sig-name descname"><span class="pre">compute_transition_beam_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.compute_transition_beam_scores" title="Permalink to this definition"></a></dt>
<dd><p>compute the transition probabilities of sequences given generation
scores and beam indices</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.config_class">
<span class="sig-name descname"><span class="pre">config_class</span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.config_class" title="Permalink to this definition"></a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">lambeq.bobcat.tagger.ChartClassifierConfig</span></code></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.constrained_beam_search">
<span class="sig-name descname"><span class="pre">constrained_beam_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constrained_beam_scorer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.generation_beam_search.ConstrainedBeamSearchScorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_utils.BeamSearchEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSearchDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.constrained_beam_search" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head using <strong>constrained beam search
decoding</strong> and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="simple">
<dt>input_ids (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>):</dt><dd><p>The sequence used as a prompt for the generation.</p>
</dd>
<dt>constrained_beam_scorer (<cite>ConstrainedBeamSearchScorer</cite>):</dt><dd><p>A derived instance of [<cite>BeamScorer</cite>] that defines how beam hypotheses are constructed, stored and
sorted during generation, while satisfying a list of positive constraints. For more information, the
documentation of [<cite>ConstrainedBeamSearchScorer</cite>] should be read.</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsProcessor</cite>]
used to modify the prediction scores of the language modeling head applied at each generation step.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>StoppingCriteriaList</cite>]. List of instances of class derived from [<cite>StoppingCriteria</cite>]
used to tell if the generation loop should stop.</p>
</dd>
<dt>logits_warper (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsWarper</cite>] used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to 20):</dt><dd><p><strong>DEPRECATED</strong>. Use <cite>logits_processor</cite> or <cite>stopping_criteria</cite> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific kwargs will be forwarded to the <cite>forward</cite> function of the model. If model is
an encoder-decoder model the kwargs should include <cite>encoder_outputs</cite>.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>[<cite>generation_utilsBeamSearchDecoderOnlyOutput</cite>], [<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>] or
<cite>torch.LongTensor</cite>: A <cite>torch.LongTensor</cite> containing the generated tokens (default behaviour) or a
[<cite>~generation_utils.BeamSearchDecoderOnlyOutput</cite>] if <cite>model.config.is_encoder_decoder=False</cite> and
<cite>return_dict_in_generate=True</cite> or a [<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>] if
<cite>model.config.is_encoder_decoder=True</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
&gt;&gt;&gt; from transformers import (
…     AutoTokenizer,
…     AutoModelForSeq2SeqLM,
…     LogitsProcessorList,
…     MinLengthLogitsProcessor,
…     ConstrainedBeamSearchScorer,
…     PhrasalConstraint,
… )
&gt;&gt;&gt; import torch</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">&quot;translate English to German: How old are you?&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run beam search using 3 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span>
<span class="gp">... </span>        <span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">constraint_str</span> <span class="o">=</span> <span class="s2">&quot;Sie&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">constraint_token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">constraint_str</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># slice to remove eos token</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="n">PhrasalConstraint</span><span class="p">(</span><span class="n">token_ids</span><span class="o">=</span><span class="n">constraint_token_ids</span><span class="p">)]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">ConstrainedBeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">constrained_beam_search</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Wie alt sind Sie?&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.cpu" title="Permalink to this definition"></a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.create_extended_attention_mask_for_decoder">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_extended_attention_mask_for_decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.create_extended_attention_mask_for_decoder" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.cuda" title="Permalink to this definition"></a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (int, optional): if specified, all parameters will be</dt><dd><p>copied to that device</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.device</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.device" title="Permalink to this definition"></a></dt>
<dd><p><cite>torch.device</cite>: The device on which the module is (assuming that all the module parameters are on the same
device).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.double" title="Permalink to this definition"></a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.dtype</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.dtype" title="Permalink to this definition"></a></dt>
<dd><p><cite>torch.dtype</cite>: The dtype of the module (assuming that all the module parameters have the same dtype).</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.dummy_inputs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dummy_inputs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.dummy_inputs" title="Permalink to this definition"></a></dt>
<dd><p><cite>Dict[str, torch.Tensor]</cite>: Dummy inputs to do a forward pass in the network.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.dump_patches" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.estimate_tokens">
<span class="sig-name descname"><span class="pre">estimate_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.estimate_tokens" title="Permalink to this definition"></a></dt>
<dd><p>Helper function to estimate the total number of tokens from the model inputs.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs (<cite>dict</cite>): The model inputs.</p>
</dd>
<dt>Returns:</dt><dd><p><cite>int</cite>: The total number of tokens.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.eval" title="Permalink to this definition"></a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.extra_repr" title="Permalink to this definition"></a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.float" title="Permalink to this definition"></a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.floating_point_ops">
<span class="sig-name descname"><span class="pre">floating_point_ops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.floating_point_ops" title="Permalink to this definition"></a></dt>
<dd><p>Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a
batch with this transformer model. Default approximation neglects the quadratic dependency on the number of
tokens (valid if <cite>12 * d_model &lt;&lt; sequence_length</cite>) as laid out in [this
paper](<a class="reference external" href="https://arxiv.org/pdf/2001.08361.pdf">https://arxiv.org/pdf/2001.08361.pdf</a>) section 2.1. Should be overridden for transformers with parameter
re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>batch_size (<cite>int</cite>):</dt><dd><p>The batch size for the forward pass.</p>
</dd>
<dt>sequence_length (<cite>int</cite>):</dt><dd><p>The number of tokens in each line of the batch.</p>
</dd>
<dt>exclude_embeddings (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>):</dt><dd><p>Whether or not to count embedding and softmax operations.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><cite>int</cite>: The number of floating-point operations.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.FloatTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.FloatTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.FloatTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.BoolTensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">ChartClassifierOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#BertForChartClassification.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.framework">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">framework</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.framework" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Str</dt>
<dd class="field-odd"><p>Identifies that this is a PyTorch model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">os.PathLike</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.from_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with <cite>model.train()</cite>.</p>
<p>The warning <em>Weights from XXX not initialized from pretrained model</em> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <em>Weights from XXX not used in YYY</em> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<dl>
<dt>Parameters:</dt><dd><dl>
<dt>pretrained_model_name_or_path (<cite>str</cite> or <cite>os.PathLike</cite>, <em>optional</em>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <cite>bert-base-uncased</cite>, or namespaced under a
user or organization name, like <cite>dbmdz/bert-base-german-cased</cite>.</p></li>
<li><p>A path to a <em>directory</em> containing model weights saved using
[<cite>~PreTrainedModel.save_pretrained</cite>], e.g., <cite>./my_model_directory/</cite>.</p></li>
<li><p>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <cite>./tf_model/model.ckpt.index</cite>). In
this case, <cite>from_tf</cite> should be set to <cite>True</cite> and a configuration object should be provided as
<cite>config</cite> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
<li><p>A path or url to a model folder containing a <em>flax checkpoint file</em> in <em>.msgpack</em> format (e.g,
<cite>./flax_model/</cite> containing <cite>flax_model.msgpack</cite>). In this case, <cite>from_flax</cite> should be set to
<cite>True</cite>.</p></li>
<li><p><cite>None</cite> if you are both providing the configuration and state dictionary (resp. with keyword
arguments <cite>config</cite> and <cite>state_dict</cite>).</p></li>
</ul>
</div></blockquote>
</dd>
<dt>model_args (sequence of positional arguments, <em>optional</em>):</dt><dd><p>All remaining positional arguments will be passed to the underlying model’s <cite>__init__</cite> method.</p>
</dd>
<dt>config (<cite>Union[PretrainedConfig, str, os.PathLike]</cite>, <em>optional</em>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>an instance of a class derived from [<cite>PretrainedConfig</cite>],</p></li>
<li><p>a string or path valid as input to [<cite>~PretrainedConfig.from_pretrained</cite>].</p></li>
</ul>
</div></blockquote>
<p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul class="simple">
<li><p>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</p></li>
<li><p>The model was saved using [<cite>~PreTrainedModel.save_pretrained</cite>] and is reloaded by supplying the
save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <cite>pretrained_model_name_or_path</cite> and a
configuration JSON file named <em>config.json</em> is found in the directory.</p></li>
</ul>
</div></blockquote>
</dd>
<dt>state_dict (<cite>Dict[str, torch.Tensor]</cite>, <em>optional</em>):</dt><dd><p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using [<cite>~PreTrainedModel.save_pretrained</cite>] and
[<cite>~PreTrainedModel.from_pretrained</cite>] is not a simpler option.</p>
</dd>
<dt>cache_dir (<cite>Union[str, os.PathLike]</cite>, <em>optional</em>):</dt><dd><p>Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p>
</dd>
<dt>from_tf (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Load the model weights from a TensorFlow checkpoint save file (see docstring of
<cite>pretrained_model_name_or_path</cite> argument).</p>
</dd>
<dt>from_flax (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Load the model weights from a Flax checkpoint save file (see docstring of
<cite>pretrained_model_name_or_path</cite> argument).</p>
</dd>
<dt>ignore_mismatched_sizes (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
checkpoint with 3 labels).</p>
</dd>
<dt>force_download (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
</dd>
<dt>resume_download (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.</p>
</dd>
<dt>proxies (<cite>Dict[str, str]</cite>, <em>optional</em>):</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <cite>{‘http’: ‘foo.bar:3128’,
‘http://hostname’: ‘foo.bar:4012’}</cite>. The proxies are used on each request.</p>
</dd>
<dt>output_loading_info(<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p>
</dd>
<dt>local_files_only(<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to only look at local files (i.e., do not try to download the model).</p>
</dd>
<dt>use_auth_token (<cite>str</cite> or <em>bool</em>, <em>optional</em>):</dt><dd><p>The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, will use the token generated
when running <cite>transformers-cli login</cite> (stored in <cite>~/.huggingface</cite>).</p>
</dd>
<dt>revision (<cite>str</cite>, <em>optional</em>, defaults to <cite>“main”</cite>):</dt><dd><p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <cite>revision</cite> can be any
identifier allowed by git.</p>
</dd>
<dt>mirror (<cite>str</cite>, <em>optional</em>):</dt><dd><p>Mirror source to accelerate downloads in China. If you are from China and have an accessibility
problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.
Please refer to the mirror site for more information.</p>
</dd>
<dt>_fast_init(<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>):</dt><dd><p>Whether or not to disable fast initialization.</p>
<p>&lt;Tip warning={true}&gt;</p>
<p>One should only disable <em>_fast_init</em> to ensure backwards compatibility with <cite>transformers.__version__ &lt;
4.6.0</cite> for seeded model initialization. This argument will be removed at the next major version. See
[pull request 11471](<a class="reference external" href="https://github.com/huggingface/transformers/pull/11471">https://github.com/huggingface/transformers/pull/11471</a>) for more information.</p>
<p>&lt;/Tip&gt;</p>
</dd>
</dl>
<p>&gt; Parameters for big model inference</p>
<dl>
<dt>low_cpu_mem_usage(<cite>bool</cite>, <em>optional</em>):</dt><dd><p>Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.
This is an experimental feature and a subject to change at any moment.</p>
</dd>
<dt>torch_dtype (<cite>str</cite> or <cite>torch.dtype</cite>, <em>optional</em>):</dt><dd><p>Override the default <cite>torch.dtype</cite> and load the model under this dtype. If <cite>“auto”</cite> is passed the dtype
will be automatically derived from the model’s weights.</p>
</dd>
<dt>device_map (<cite>str</cite> or <cite>Dict[str, Union[int, str, torch.device]]</cite>, <em>optional</em>):</dt><dd><p>A map that specifies where each submodule should go. It doesn’t need to be refined to each
parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
same device.</p>
<p>To have Accelerate compute the most optimized <cite>device_map</cite> automatically, set <cite>device_map=”auto”</cite>.</p>
</dd>
<dt>max_memory (<cite>Dict</cite>, <em>optional</em>):</dt><dd><p>A dictionary device identifier to maximum memory. Will default to the maximum memory available for each
GPU and the available CPU RAM if unset.</p>
</dd>
<dt>offload_folder (<cite>str</cite> or <cite>os.PathLike</cite>, <em>optional</em>):</dt><dd><p>If the <cite>device_map</cite> contains any value <cite>“disk”</cite>, the folder where we will offload weights.</p>
</dd>
<dt>offload_state_dict (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>If <cite>True</cite>, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU
RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit.</p>
</dd>
<dt>kwargs (remaining dictionary of keyword arguments, <em>optional</em>):</dt><dd><p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<cite>output_attentions=True</cite>). Behaves differently depending on whether a <cite>config</cite> is provided or
automatically loaded:</p>
<blockquote>
<div><ul class="simple">
<li><p>If a configuration is provided with <cite>config</cite>, <cite>**kwargs</cite> will be directly passed to the
underlying model’s <cite>__init__</cite> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <cite>kwargs</cite> will be first passed to the configuration class
initialization function ([<cite>~PretrainedConfig.from_pretrained</cite>]). Each key of <cite>kwargs</cite> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <cite>kwargs</cite> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model’s <cite>__init__</cite> function.</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<p>&lt;Tip&gt;</p>
<p>Passing <cite>use_auth_token=True`</cite> is required when you want to use a private model.</p>
<p>&lt;/Tip&gt;</p>
<p>&lt;Tip&gt;</p>
<p>Activate the special [“offline-mode”](<a class="reference external" href="https://huggingface.co/transformers/installation.html#offline-mode">https://huggingface.co/transformers/installation.html#offline-mode</a>) to
use this method in a firewalled environment.</p>
<p>&lt;/Tip&gt;</p>
<p>Examples:</p>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a>python
&gt;&gt;&gt; from transformers import BertConfig, BertModel</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Model was saved using *save_pretrained(&#39;./test/saved_model/&#39;)* (for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_model_config.json&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_checkpoint.ckpt.index&quot;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a Flax checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">from_flax</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">```</span>
</pre></div>
</div>
<ul class="simple">
<li><p><cite>low_cpu_mem_usage</cite> algorithm:</p></li>
</ul>
<p>This is an experimental function that loads the model using ~1x model size CPU memory</p>
<p>Here is how it works:</p>
<ol class="arabic simple">
<li><p>save which state_dict keys we have</p></li>
<li><p>drop state_dict before the model is created, since the latter takes 1x model size CPU memory</p></li>
</ol>
<p>3. after the model has been instantiated switch to the meta device all params/buffers that
are going to be replaced from the loaded state_dict
4. load state_dict 2nd time
5. replace the params/buffers from the state_dict</p>
<p>Currently, it can’t handle deepspeed ZeRO stage 3 and ignores loading errors</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">do_sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_beams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">typical_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repetition_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bad_words_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_words_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_repeat_ngram_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_no_repeat_ngram_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_return_sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_time</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_new_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_start_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_beam_groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">diversity_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix_allowed_tokens_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">renormalize_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_beam_constraints.Constraint</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forced_bos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forced_eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_invalid_values</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exponential_decay_length_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_utils.GreedySearchEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.GreedySearchDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.SampleEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.SampleDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSearchEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSearchDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSampleEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.BeamSampleDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.generate" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:</p>
<blockquote>
<div><ul class="simple">
<li><p><em>greedy decoding</em> by calling [<cite>~generation_utils.GenerationMixin.greedy_search</cite>] if <cite>num_beams=1</cite> and
<cite>do_sample=False</cite>.</p></li>
<li><p><em>multinomial sampling</em> by calling [<cite>~generation_utils.GenerationMixin.sample</cite>] if <cite>num_beams=1</cite> and
<cite>do_sample=True</cite>.</p></li>
<li><p><em>beam-search decoding</em> by calling [<cite>~generation_utils.GenerationMixin.beam_search</cite>] if <cite>num_beams&gt;1</cite> and
<cite>do_sample=False</cite>.</p></li>
<li><p><em>beam-search multinomial sampling</em> by calling [<cite>~generation_utils.GenerationMixin.beam_sample</cite>] if
<cite>num_beams&gt;1</cite> and <cite>do_sample=True</cite>.</p></li>
<li><p><em>diverse beam-search decoding</em> by calling [<cite>~generation_utils.GenerationMixin.group_beam_search</cite>], if
<cite>num_beams&gt;1</cite> and <cite>num_beam_groups&gt;1</cite>.</p></li>
<li><p><em>constrained beam-search decoding</em> by calling
[<cite>~generation_utils.GenerationMixin.constrained_beam_search</cite>], if <cite>constraints!=None</cite> or
<cite>force_words_ids!=None</cite>.</p></li>
</ul>
</div></blockquote>
<p>&lt;Tip warning={true}&gt;</p>
<p>Apart from <cite>inputs</cite>, all the arguments below will default to the value of the attribute of the same name as
defined in the model’s config (<cite>config.json</cite>) which in turn defaults to the
[<cite>~modeling_utils.PretrainedConfig</cite>] of the model.</p>
<p>&lt;/Tip&gt;</p>
<p>Most of these parameters are explained in more detail in [this blog
post](<a class="reference external" href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a>).</p>
<dl>
<dt>Parameters:</dt><dd><dl class="simple">
<dt>inputs (<cite>torch.Tensor</cite> of varying shape depending on the modality, <em>optional</em>):</dt><dd><p>The sequence used as a prompt for the generation or as model inputs to the encoder. If <cite>None</cite> the
method initializes it with <cite>bos_token_id</cite> and a batch size of 1. For decoder-only models <cite>inputs</cite>
should of in the format of <cite>input_ids</cite>. For encoder-decoder models <em>inputs</em> can represent any of
<cite>input_ids</cite>, <cite>input_values</cite>, <cite>input_features</cite>, or <cite>pixel_values</cite>.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to <cite>model.config.max_length</cite>):</dt><dd><p>The maximum length of the sequence to be generated.</p>
</dd>
<dt>max_new_tokens (<cite>int</cite>, <em>optional</em>, defaults to None):</dt><dd><p>The maximum numbers of tokens to generate, ignore the current number of tokens. Use either
<cite>max_new_tokens</cite> or <cite>max_length</cite> but not both, they serve the same purpose.</p>
</dd>
<dt>min_length (<cite>int</cite>, <em>optional</em>, defaults to 10):</dt><dd><p>The minimum length of the sequence to be generated.</p>
</dd>
<dt>do_sample (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to use sampling ; use greedy decoding otherwise.</p>
</dd>
<dt>early_stopping (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to stop the beam search when at least <cite>num_beams</cite> sentences are finished per batch or not.</p>
</dd>
<dt>num_beams (<cite>int</cite>, <em>optional</em>, defaults to 1):</dt><dd><p>Number of beams for beam search. 1 means no beam search.</p>
</dd>
<dt>temperature (<cite>float</cite>, <em>optional</em>, defaults to 1.0):</dt><dd><p>The value used to module the next token probabilities.</p>
</dd>
<dt>top_k (<cite>int</cite>, <em>optional</em>, defaults to 50):</dt><dd><p>The number of highest probability vocabulary tokens to keep for top-k-filtering.</p>
</dd>
<dt>top_p (<cite>float</cite>, <em>optional</em>, defaults to 1.0):</dt><dd><p>If set to float &lt; 1, only the most probable tokens with probabilities that add up to <cite>top_p</cite> or higher
are kept for generation.</p>
</dd>
<dt>typical_p (<cite>float</cite>, <em>optional</em>, defaults to 1.0):</dt><dd><p>The amount of probability mass from the original distribution to be considered in typical decoding. If
set to 1.0 it takes no effect. See [this paper](<a class="reference external" href="https://arxiv.org/pdf/2202.00666.pdf">https://arxiv.org/pdf/2202.00666.pdf</a>) for more details.</p>
</dd>
<dt>repetition_penalty (<cite>float</cite>, <em>optional</em>, defaults to 1.0):</dt><dd><p>The parameter for repetition penalty. 1.0 means no penalty. See [this
paper](<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>) for more details.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>bos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>beginning-of-sequence</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>length_penalty (<cite>float</cite>, <em>optional</em>, defaults to 1.0):</dt><dd><p>Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.
0.0 means no penalty. Set to values &lt; 0.0 in order to encourage the model to generate longer
sequences, to a value &gt; 0.0 in order to encourage the model to produce shorter sequences.</p>
</dd>
<dt>no_repeat_ngram_size (<cite>int</cite>, <em>optional</em>, defaults to 0):</dt><dd><p>If set to int &gt; 0, all ngrams of that size can only occur once.</p>
</dd>
<dt>encoder_no_repeat_ngram_size (<cite>int</cite>, <em>optional</em>, defaults to 0):</dt><dd><p>If set to int &gt; 0, all ngrams of that size that occur in the <cite>encoder_input_ids</cite> cannot occur in the
<cite>decoder_input_ids</cite>.</p>
</dd>
<dt>bad_words_ids(<cite>List[List[int]]</cite>, <em>optional</em>):</dt><dd><p>List of token ids that are not allowed to be generated. In order to get the token ids of the words that
should not appear in the generated text, use <cite>tokenizer(bad_words, add_prefix_space=True,
add_special_tokens=False).input_ids</cite>.</p>
</dd>
<dt>force_words_ids(<cite>List[List[int]]</cite> or <cite>List[List[List[int]]]</cite>, <em>optional</em>):</dt><dd><p>List of token ids that must be generated. If given a <cite>List[List[int]]</cite>, this is treated as a simple
list of words that must be included, the opposite to <cite>bad_words_ids</cite>. If given <cite>List[List[List[int]]]</cite>,
this triggers a [disjunctive constraint](<a class="reference external" href="https://github.com/huggingface/transformers/issues/14081">https://github.com/huggingface/transformers/issues/14081</a>),
where one can allow different forms of each word.</p>
</dd>
<dt>num_return_sequences(<cite>int</cite>, <em>optional</em>, defaults to 1):</dt><dd><p>The number of independently computed returned sequences for each element in the batch.</p>
</dd>
<dt>max_time(<cite>float</cite>, <em>optional</em>, defaults to None):</dt><dd><p>The maximum amount of time you allow the computation to run for in seconds. generation will still
finish the current pass after allocated time has been passed.</p>
</dd>
<dt>attention_mask (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>, <em>optional</em>):</dt><dd><p>Mask to avoid performing attention on padding token indices. Mask values are in <cite>[0, 1]</cite>, 1 for tokens
that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape
as <cite>input_ids</cite> that masks the pad token. [What are attention masks?](../glossary#attention-mask)</p>
</dd>
<dt>decoder_start_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.</p>
</dd>
<dt>use_cache: (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>):</dt><dd><p>Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.</p>
</dd>
<dt>num_beam_groups (<cite>int</cite>, <em>optional</em>, defaults to 1):</dt><dd><p>Number of groups to divide <cite>num_beams</cite> into in order to ensure diversity among different groups of
beams. [this paper](<a class="reference external" href="https://arxiv.org/pdf/1610.02424.pdf">https://arxiv.org/pdf/1610.02424.pdf</a>) for more details.</p>
</dd>
<dt>diversity_penalty (<cite>float</cite>, <em>optional</em>, defaults to 0.0):</dt><dd><p>This value is subtracted from a beam’s score if it generates a token same as any beam from other group
at a particular time. Note that <cite>diversity_penalty</cite> is only effective if <cite>group beam search</cite> is
enabled.</p>
</dd>
<dt>prefix_allowed_tokens_fn (<cite>Callable[[int, torch.Tensor], List[int]]</cite>, <em>optional</em>):</dt><dd><p>If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <cite>batch_id</cite> and
<cite>input_ids</cite>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <cite>batch_id</cite> and the previously generated tokens <cite>inputs_ids</cite>. This argument is useful
for constrained generation conditioned on the prefix, as described in [Autoregressive Entity
Retrieval](<a class="reference external" href="https://arxiv.org/abs/2010.00904">https://arxiv.org/abs/2010.00904</a>).</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>Custom logits processors that complement the default logits processors built from arguments and a
model’s config. If a logit processor is passed that is already created with the arguments or a model’s
config an error is thrown. This feature is intended for advanced users.</p>
</dd>
<dt>renormalize_logits: (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to renormalize the logits after applying all the logits processors or warpers (including the
custom ones). It’s highly recommended to set this flag to <cite>True</cite> as the search algorithms suppose the
score logits are normalized but some logit processors or warpers break the normalization.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>Custom stopping criteria that complement the default stopping criteria built from arguments and a
model’s config. If a stopping criteria is passed that is already created with the arguments or a
model’s config an error is thrown. This feature is intended for advanced users.</p>
</dd>
<dt>constraints (<cite>List[Constraint]</cite>, <em>optional</em>):</dt><dd><p>Custom constraints that can be added to the generation to ensure that the output will contain the use
of certain tokens as defined by <cite>Constraint</cite> objects, in the most sensible way possible.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>forced_bos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the token to force as the first generated token after the <cite>decoder_start_token_id</cite>. Useful
for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be
the target language token.</p>
</dd>
<dt>forced_eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the token to force as the last generated token when <cite>max_length</cite> is reached.</p>
</dd>
<dt>remove_invalid_values (<cite>bool</cite>, <em>optional</em>):</dt><dd><p>Whether to remove possible <em>nan</em> and <em>inf</em> outputs of the model to prevent the generation method to
crash. Note that using <cite>remove_invalid_values</cite> can slow down generation.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>exponential_decay_length_penalty (<cite>tuple(int, float)</cite>, <em>optional</em>):</dt><dd><p>This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been
generated. The tuple shall consist of: <cite>(start_index, decay_factor)</cite> where <cite>start_index</cite> indicates
where penalty starts and <cite>decay_factor</cite> represents the factor of exponential decay</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific kwargs will be forwarded to the <cite>forward</cite> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with <em>decoder_</em>.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>[<cite>~utils.ModelOutput</cite>] or <cite>torch.LongTensor</cite>: A [<cite>~utils.ModelOutput</cite>] (if <cite>return_dict_in_generate=True</cite>
or when <cite>config.return_dict_in_generate=True</cite>) or a <cite>torch.FloatTensor</cite>.</p>
<blockquote>
<div><p>If the model is <em>not</em> an encoder-decoder model (<cite>model.config.is_encoder_decoder=False</cite>), the possible
[<cite>~utils.ModelOutput</cite>] types are:</p>
<blockquote>
<div><ul class="simple">
<li><p>[<cite>~generation_utils.GreedySearchDecoderOnlyOutput</cite>],</p></li>
<li><p>[<cite>~generation_utils.SampleDecoderOnlyOutput</cite>],</p></li>
<li><p>[<cite>~generation_utils.BeamSearchDecoderOnlyOutput</cite>],</p></li>
<li><p>[<cite>~generation_utils.BeamSampleDecoderOnlyOutput</cite>]</p></li>
</ul>
</div></blockquote>
<p>If the model is an encoder-decoder model (<cite>model.config.is_encoder_decoder=True</cite>), the possible
[<cite>~utils.ModelOutput</cite>] types are:</p>
<blockquote>
<div><ul class="simple">
<li><p>[<cite>~generation_utils.GreedySearchEncoderDecoderOutput</cite>],</p></li>
<li><p>[<cite>~generation_utils.SampleEncoderDecoderOutput</cite>],</p></li>
<li><p>[<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>],</p></li>
<li><p>[<cite>~generation_utils.BeamSampleEncoderDecoderOutput</cite>]</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</dd>
</dl>
<p>Examples:</p>
<p>Greedy Decoding:</p>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>python
&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Today I believe we can finally&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># generate up to 30 tokens</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\n&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
<p>Multinomial Sampling:</p>
<p><a href="#id21"><span class="problematic" id="id22">``</span></a><a href="#id23"><span class="problematic" id="id24">`</span></a>python
&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForCausalLM
&gt;&gt;&gt; import torch</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Today I believe we can finally&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># sample up to 30 tokens</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Today I believe we can finally get rid of discrimination,&quot; said Rep. Mark Pocan (D-Wis.).\n\n&quot;Just look at the&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
<p>Beam-search decoding:</p>
<p><a href="#id25"><span class="problematic" id="id26">``</span></a><a href="#id27"><span class="problematic" id="id28">`</span></a>python
&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;Paris is one of the densest populated areas in Europe.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Paris ist eines der dichtesten besiedelten Gebiete Europas.&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_buffer" title="Permalink to this definition"></a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>target: The fully-qualified string name of the buffer</dt><dd><p>to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>torch.Tensor: The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>AttributeError: If the target string references an invalid</dt><dd><p>path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_extended_attention_mask">
<span class="sig-name descname"><span class="pre">get_extended_attention_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">attention_mask:</span> <span class="pre">torch.Tensor,</span> <span class="pre">input_shape:</span> <span class="pre">typing.Tuple[int],</span> <span class="pre">device:</span> <span class="pre">&lt;property</span> <span class="pre">object</span> <span class="pre">at</span> <span class="pre">0x7f3f1c70a130&gt;</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_extended_attention_mask" title="Permalink to this definition"></a></dt>
<dd><p>Makes broadcastable attention and causal masks so that future and masked tokens are ignored.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>attention_mask (<cite>torch.Tensor</cite>):</dt><dd><p>Mask with ones indicating tokens to attend to, zeros for tokens to ignore.</p>
</dd>
<dt>input_shape (<cite>Tuple[int]</cite>):</dt><dd><p>The shape of the input to the model.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><cite>torch.Tensor</cite> The extended attention mask, with a the same dtype as <cite>attention_mask.dtype</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_extra_state" title="Permalink to this definition"></a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.set_extra_state" title="lambeq.bobcat.BertForChartClassification.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>object: Any extra state to store in the module’s state_dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_head_mask">
<span class="sig-name descname"><span class="pre">get_head_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_hidden_layers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_attention_chunked</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_head_mask" title="Permalink to this definition"></a></dt>
<dd><p>Prepare the head mask if needed.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>head_mask (<cite>torch.Tensor</cite> with shape <cite>[num_heads]</cite> or <cite>[num_hidden_layers x num_heads]</cite>, <em>optional</em>):</dt><dd><p>The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).</p>
</dd>
<dt>num_hidden_layers (<cite>int</cite>):</dt><dd><p>The number of hidden layers in the model.</p>
</dd>
<dt>is_attention_chunked: (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not the attentions scores are computed by chunks or not.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><cite>torch.Tensor</cite> with shape <cite>[num_hidden_layers x batch x num_heads x seq_length x seq_length]</cite> or list with
<cite>[None]</cite> for each layer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_input_embeddings">
<span class="sig-name descname"><span class="pre">get_input_embeddings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_input_embeddings" title="Permalink to this definition"></a></dt>
<dd><p>Returns the model’s input embeddings.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p><cite>nn.Module</cite>: A torch module mapping vocabulary to hidden states.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_output_embeddings">
<span class="sig-name descname"><span class="pre">get_output_embeddings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_output_embeddings" title="Permalink to this definition"></a></dt>
<dd><p>Returns the model’s output embeddings.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p><cite>nn.Module</cite>: A torch module mapping hidden states to vocabulary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.parameter.Parameter</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_parameter" title="Permalink to this definition"></a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>target: The fully-qualified string name of the Parameter</dt><dd><p>to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>torch.nn.Parameter: The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>AttributeError: If the target string references an invalid</dt><dd><p>path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_position_embeddings">
<span class="sig-name descname"><span class="pre">get_position_embeddings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.sparse.Embedding</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.sparse.Embedding</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_position_embeddings" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.get_submodule" title="Permalink to this definition"></a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>target: The fully-qualified string name of the submodule</dt><dd><p>to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>torch.nn.Module: The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>AttributeError: If the target string references an invalid</dt><dd><p>path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.gradient_checkpointing_disable">
<span class="sig-name descname"><span class="pre">gradient_checkpointing_disable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.gradient_checkpointing_disable" title="Permalink to this definition"></a></dt>
<dd><p>Deactivates gradient checkpointing for the current model.</p>
<p>Note that in other frameworks this feature can be referred to as “activation checkpointing” or “checkpoint
activations”.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.gradient_checkpointing_enable">
<span class="sig-name descname"><span class="pre">gradient_checkpointing_enable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.gradient_checkpointing_enable" title="Permalink to this definition"></a></dt>
<dd><p>Activates gradient checkpointing for the current model.</p>
<p>Note that in other frameworks this feature can be referred to as “activation checkpointing” or “checkpoint
activations”.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.greedy_search">
<span class="sig-name descname"><span class="pre">greedy_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_utils.GreedySearchEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.GreedySearchDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.greedy_search" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head using <strong>greedy decoding</strong> and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</p>
<p>Parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>input_ids (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>):</dt><dd><p>The sequence used as a prompt for the generation.</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsProcessor</cite>]
used to modify the prediction scores of the language modeling head applied at each generation step.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>StoppingCriteriaList</cite>]. List of instances of class derived from [<cite>StoppingCriteria</cite>]
used to tell if the generation loop should stop.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to 20):</dt><dd><p><strong>DEPRECATED</strong>. Use <cite>logits_processor</cite> or <cite>stopping_criteria</cite> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific keyword arguments will be forwarded to the <cite>forward</cite> function of the model.
If model is an encoder-decoder model the kwargs should include <cite>encoder_outputs</cite>.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Return:</dt><dd><p>[<cite>~generation_utils.GreedySearchDecoderOnlyOutput</cite>], [<cite>~generation_utils.GreedySearchEncoderDecoderOutput</cite>]
or <cite>torch.LongTensor</cite>: A <cite>torch.LongTensor</cite> containing the generated tokens (default behaviour) or a
[<cite>~generation_utils.GreedySearchDecoderOnlyOutput</cite>] if <cite>model.config.is_encoder_decoder=False</cite> and
<cite>return_dict_in_generate=True</cite> or a [<cite>~generation_utils.GreedySearchEncoderDecoderOutput</cite>] if
<cite>model.config.is_encoder_decoder=True</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a>python
&gt;&gt;&gt; from transformers import (
…     AutoTokenizer,
…     AutoModelForCausalLM,
…     LogitsProcessorList,
…     MinLengthLogitsProcessor,
…     StoppingCriteriaList,
…     MaxLengthCriteria,
… )</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_prompt</span> <span class="o">=</span> <span class="s2">&quot;It might be possible to&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">StoppingCriteriaList</span><span class="p">([</span><span class="n">MaxLengthCriteria</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">greedy_search</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">stopping_criteria</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&quot;It might be possible to get a better understanding of the nature of the problem, but it&#39;s not&quot;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.group_beam_search">
<span class="sig-name descname"><span class="pre">group_beam_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_scorer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.generation_beam_search.BeamScorer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.group_beam_search" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head using <strong>diverse beam search
decoding</strong> and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</p>
<p>Parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>input_ids (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>):</dt><dd><p>The sequence used as a prompt for the generation.</p>
</dd>
<dt>beam_scorer (<cite>BeamScorer</cite>):</dt><dd><p>An derived instance of [<cite>BeamScorer</cite>] that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of [<cite>BeamScorer</cite>] should be read.</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsProcessor</cite>]
used to modify the prediction scores of the language modeling head applied at each generation step.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>StoppingCriteriaList</cite>]. List of instances of class derived from [<cite>StoppingCriteria</cite>]
used to tell if the generation loop should stop.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to 20):</dt><dd><p><strong>DEPRECATED</strong>. Use <cite>logits_processor</cite> or <cite>stopping_criteria</cite> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific kwargs that will be forwarded to the <cite>forward</cite> function of the model. If
model is an encoder-decoder model the kwargs should include <cite>encoder_outputs</cite>.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Return:</dt><dd><p>[<cite>~generation_utils.BeamSearchDecoderOnlyOutput</cite>], [<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>] or
<cite>torch.LongTensor</cite>: A <cite>torch.LongTensor</cite> containing the generated tokens (default behaviour) or a
[<cite>~generation_utils.BeamSearchDecoderOnlyOutput</cite>] if [<cite>~generation_utils.BeamSearchDecoderOnlyOutput</cite>] if
<cite>model.config.is_encoder_decoder=False</cite> and <cite>return_dict_in_generate=True</cite> or a
[<cite>~generation_utils.BeamSearchEncoderDecoderOutput</cite>] if <cite>model.config.is_encoder_decoder=True</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>python
&gt;&gt;&gt; from transformers import (
…     AutoTokenizer,
…     AutoModelForSeq2SeqLM,
…     LogitsProcessorList,
…     MinLengthLogitsProcessor,
…     HammingDiversityLogitsProcessor,
…     BeamSearchScorer,
… )
&gt;&gt;&gt; import torch</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;t5-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_str</span> <span class="o">=</span> <span class="s2">&quot;translate English to German: How old are you?&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">encoder_input_str</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># lets run diverse beam search using 6 beams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_beams</span> <span class="o">=</span> <span class="mi">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># define decoder start token ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">num_beams</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># add encoder_outputs to model keyword arguments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;encoder_outputs&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">get_encoder</span><span class="p">()(</span>
<span class="gp">... </span>        <span class="n">encoder_input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate beam scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beam_scorer</span> <span class="o">=</span> <span class="n">BeamSearchScorer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_length</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beams</span><span class="o">=</span><span class="n">num_beams</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">device</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">HammingDiversityLogitsProcessor</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">group_beam_search</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span><span class="p">,</span> <span class="n">beam_scorer</span><span class="p">,</span> <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Wie alt bist du?&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.half" title="Permalink to this definition"></a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.init_weights">
<span class="sig-name descname"><span class="pre">init_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.init_weights" title="Permalink to this definition"></a></dt>
<dd><p>If needed prunes and maybe initializes weights.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.invert_attention_mask">
<span class="sig-name descname"><span class="pre">invert_attention_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.invert_attention_mask" title="Permalink to this definition"></a></dt>
<dd><p>Invert an attention mask (e.g., switches 0. and 1.).</p>
<dl class="simple">
<dt>Args:</dt><dd><p>encoder_attention_mask (<cite>torch.Tensor</cite>): An attention mask.</p>
</dd>
<dt>Returns:</dt><dd><p><cite>torch.Tensor</cite>: The inverted attention mask.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.ipu">
<span class="sig-name descname"><span class="pre">ipu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.ipu" title="Permalink to this definition"></a></dt>
<dd><p>Moves all model parameters and buffers to the IPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on IPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>device (int, optional): if specified, all parameters will be</dt><dd><p>copied to that device</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.is_gradient_checkpointing">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_gradient_checkpointing</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.is_gradient_checkpointing" title="Permalink to this definition"></a></dt>
<dd><p>Whether gradient checkpointing is activated for this model or not.</p>
<p>Note that in other frameworks this feature can be referred to as “activation checkpointing” or “checkpoint
activations”.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.is_parallelizable">
<span class="sig-name descname"><span class="pre">is_parallelizable</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.is_parallelizable" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.load_state_dict" title="Permalink to this definition"></a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>state_dict (dict): a dict containing parameters and</dt><dd><p>persistent buffers.</p>
</dd>
<dt>strict (bool, optional): whether to strictly enforce that the keys</dt><dd><p>in <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields:</dt><dd><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</dd>
</dl>
</dd>
<dt>Note:</dt><dd><p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.load_state_dict" title="lambeq.bobcat.BertForChartClassification.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.load_tf_weights">
<span class="sig-name descname"><span class="pre">load_tf_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf_checkpoint_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.load_tf_weights" title="Permalink to this definition"></a></dt>
<dd><p>Load tf checkpoints in a pytorch model.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.main_input_name">
<span class="sig-name descname"><span class="pre">main_input_name</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'input_ids'</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.main_input_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.modules" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>Module: a module in the network</p>
</dd>
<dt>Note:</dt><dd><p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.named_buffers" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl>
<dt>Args:</dt><dd><p>prefix (str): prefix to prepend to all buffer names.
recurse (bool): if True, then yields buffers of this module</p>
<blockquote>
<div><p>and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</div></blockquote>
</dd>
<dt>Yields:</dt><dd><p>(string, torch.Tensor): Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.named_children" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="simple">
<dt>Yields:</dt><dd><p>(string, Module): Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Set</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.named_modules" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl>
<dt>Args:</dt><dd><p>memo: a memo to store the set of modules already added to the result
prefix: a prefix that will be added to the name of the module
remove_duplicate: whether to remove the duplicated module instances in the result</p>
<blockquote>
<div><p>or not</p>
</div></blockquote>
</dd>
<dt>Yields:</dt><dd><p>(string, Module): Tuple of name and module</p>
</dd>
<dt>Note:</dt><dd><p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.named_parameters" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl>
<dt>Args:</dt><dd><p>prefix (str): prefix to prepend to all parameter names.
recurse (bool): if True, then yields parameters of this module</p>
<blockquote>
<div><p>and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</div></blockquote>
</dd>
<dt>Yields:</dt><dd><p>(string, Parameter): Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.num_parameters">
<span class="sig-name descname"><span class="pre">num_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">only_trainable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.num_parameters" title="Permalink to this definition"></a></dt>
<dd><p>Get number of (optionally, trainable or non-embeddings) parameters in the module.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>only_trainable (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return only the number of trainable parameters</p>
</dd>
<dt>exclude_embeddings (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return only the number of non-embeddings parameters</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><cite>int</cite>: The number of parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.parameters" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>recurse (bool): if True, then yields parameters of this module</dt><dd><p>and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
</dl>
</dd>
<dt>Yields:</dt><dd><p>Parameter: module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.post_init">
<span class="sig-name descname"><span class="pre">post_init</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.post_init" title="Permalink to this definition"></a></dt>
<dd><p>A method executed at the end of each Transformer model initialization, to execute code that needs the model’s
modules properly initialized (such as weight initialization).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.prepare_inputs_for_generation">
<span class="sig-name descname"><span class="pre">prepare_inputs_for_generation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.prepare_inputs_for_generation" title="Permalink to this definition"></a></dt>
<dd><p>Implement in subclasses of [<cite>PreTrainedModel</cite>] for custom behavior to prepare inputs in the generate method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.prune_heads">
<span class="sig-name descname"><span class="pre">prune_heads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">heads_to_prune</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.prune_heads" title="Permalink to this definition"></a></dt>
<dd><p>Prunes heads of the base model.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>heads_to_prune (<cite>Dict[int, List[int]]</cite>):</dt><dd><p>Dictionary with keys being selected layer indices (<cite>int</cite>) and associated values being the list of heads
to prune in said layer (list of <cite>int</cite>). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on
layer 1 and heads 2 and 3 on layer 2.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.push_to_hub">
<span class="sig-name descname"><span class="pre">push_to_hub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repo_path_or_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repo_url</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_temp_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">commit_message</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'add</span> <span class="pre">model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">organization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">private</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_auth_token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_shard_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'10GB'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_card_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.push_to_hub" title="Permalink to this definition"></a></dt>
<dd><p>Upload the model files to the 🤗 Model Hub while synchronizing a local clone of the repo in <cite>repo_path_or_name</cite>.</p>
<dl>
<dt>Parameters:</dt><dd><dl>
<dt>repo_path_or_name (<cite>str</cite>, <em>optional</em>):</dt><dd><p>Can either be a repository name for your model in the Hub or a path to a local folder (in which case
the repository will have the name of that local folder). If not specified, will default to the name
given by <cite>repo_url</cite> and a local directory with that name will be created.</p>
</dd>
<dt>repo_url (<cite>str</cite>, <em>optional</em>):</dt><dd><p>Specify this in case you want to push to an existing repository in the hub. If unspecified, a new
repository will be created in your namespace (unless you specify an <cite>organization</cite>) with <cite>repo_name</cite>.</p>
</dd>
<dt>use_temp_dir (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to clone the distant repo in a temporary directory or in <cite>repo_path_or_name</cite> inside the
current working directory. This will slow things down if you are making changes in an existing repo
since you will need to clone the repo before every push.</p>
</dd>
<dt>commit_message (<cite>str</cite>, <em>optional</em>, defaults to <cite>“add model”</cite>):</dt><dd><p>Message to commit while pushing.</p>
</dd>
<dt>organization (<cite>str</cite>, <em>optional</em>):</dt><dd><p>Organization in which you want to push your {object} (you must be a member of this organization).</p>
</dd>
<dt>private (<cite>bool</cite>, <em>optional</em>):</dt><dd><p>Whether or not the repository created should be private (requires a paying subscription).</p>
</dd>
<dt>use_auth_token (<cite>bool</cite> or <cite>str</cite>, <em>optional</em>):</dt><dd><p>The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, will use the token generated
when running <cite>transformers-cli login</cite> (stored in <cite>~/.huggingface</cite>). Will default to <cite>True</cite> if
<cite>repo_url</cite> is not specified.</p>
</dd>
<dt>max_shard_size (<cite>int</cite> or <cite>str</cite>, <em>optional</em>, defaults to <cite>“10GB”</cite>):</dt><dd><p>The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size
lower than this size. If expressed as a string, needs to be digits followed by a unit (like <cite>“5MB”</cite>).</p>
<p>&lt;Tip warning={true}&gt;</p>
<p>If a single weight of the model is bigger than <cite>max_shard_size</cite>, it will be in its own checkpoint shard
which will be bigger than <cite>max_shard_size</cite>.</p>
<p>&lt;/Tip&gt;</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p><cite>str</cite>: The url of the commit of your {object} in the given repository.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id37"><span class="problematic" id="id38">``</span></a><a href="#id39"><span class="problematic" id="id40">`</span></a>python
from transformers import AutoModel</p>
<p>model = AutoModel.from_pretrained(“bert-base-cased”)</p>
<p># Push the model to your namespace with the name “my-finetuned-bert” and have a local clone in the
# <em>my-finetuned-bert</em> folder.
model.push_to_hub(“my-finetuned-bert”)</p>
<p># Push the model to your namespace with the name “my-finetuned-bert” with no local clone.
model.push_to_hub(“my-finetuned-bert”, use_temp_dir=True)</p>
<p># Push the model to an organization with the name “my-finetuned-bert” and have a local clone in the
# <em>my-finetuned-bert</em> folder.
model.push_to_hub(“my-finetuned-bert”, organization=”huggingface”)</p>
<p># Make a change to an existing repo that has been cloned locally in <em>my-finetuned-bert</em>.
model.push_to_hub(“my-finetuned-bert”, repo_url=”<a class="reference external" href="https://huggingface.co/sgugger/my-finetuned-bert">https://huggingface.co/sgugger/my-finetuned-bert</a>”)
<a href="#id41"><span class="problematic" id="id42">``</span></a><a href="#id43"><span class="problematic" id="id44">`</span></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.utils.hooks.RemovableHandle</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_backward_hook" title="Permalink to this definition"></a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_buffer" title="Permalink to this definition"></a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>name (string): name of the buffer. The buffer can be accessed</dt><dd><p>from this module using the given name</p>
</dd>
<dt>tensor (Tensor or None): buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations</dt><dd><p>that run on buffers, such as <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.cuda" title="lambeq.bobcat.BertForChartClassification.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
</dd>
<dt>persistent (bool): whether the buffer is part of this module’s</dt><dd><p><a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_for_auto_class">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">register_for_auto_class</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">auto_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'AutoModel'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_for_auto_class" title="Permalink to this definition"></a></dt>
<dd><p>Register this class with a given auto class. This should only be used for custom models as the ones in the
library are already mapped with an auto class.</p>
<p>&lt;Tip warning={true}&gt;</p>
<p>This API is experimental and may have some slight breaking changes in the next releases.</p>
<p>&lt;/Tip&gt;</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>auto_class (<cite>str</cite> or <cite>type</cite>, <em>optional</em>, defaults to <cite>“AutoModel”</cite>):</dt><dd><p>The auto class to register this new model with.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.utils.hooks.RemovableHandle</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_forward_hook" title="Permalink to this definition"></a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.forward" title="lambeq.bobcat.BertForChartClassification.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.forward" title="lambeq.bobcat.BertForChartClassification.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.utils.hooks.RemovableHandle</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_forward_pre_hook" title="Permalink to this definition"></a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.forward" title="lambeq.bobcat.BertForChartClassification.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.utils.hooks.RemovableHandle</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_full_backward_hook" title="Permalink to this definition"></a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_load_state_dict_post_hook">
<span class="sig-name descname"><span class="pre">register_load_state_dict_post_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_load_state_dict_post_hook" title="Permalink to this definition"></a></dt>
<dd><p>Registers a post hook to be run after module’s <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>
is called.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, incompatible_keys) -&gt; None</p>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">module</span></code> argument is the current module that this hook is registered
on, and the <code class="docutils literal notranslate"><span class="pre">incompatible_keys</span></code> argument is a <code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> consisting
of attributes <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code>. <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code>
is a <code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">str</span></code> containing the missing keys and
<code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> is a <code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">str</span></code> containing the unexpected keys.</p>
<p>The given incompatible_keys can be modified inplace if needed.</p>
<p>Note that the checks performed when calling <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.load_state_dict" title="lambeq.bobcat.BertForChartClassification.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> with
<code class="docutils literal notranslate"><span class="pre">strict=True</span></code> are affected by modifications the hook makes to
<code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> or <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code>, as expected. Additions to either
set of keys will result in an error being thrown when <code class="docutils literal notranslate"><span class="pre">strict=True</span></code>, and
clearning out both missing and unexpected keys will avoid an error.</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="simple">
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code>:</dt><dd><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_module" title="Permalink to this definition"></a></dt>
<dd><p>Alias for <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.add_module" title="lambeq.bobcat.BertForChartClassification.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.parameter.Parameter</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.register_parameter" title="Permalink to this definition"></a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>name (string): name of the parameter. The parameter can be accessed</dt><dd><p>from this module using the given name</p>
</dd>
<dt>param (Parameter or None): parameter to be added to the module. If</dt><dd><p><code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.cuda" title="lambeq.bobcat.BertForChartClassification.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="lambeq.bobcat.BertForChartClassification.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.requires_grad_" title="Permalink to this definition"></a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>requires_grad (bool): whether autograd should record operations on</dt><dd><p>parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.reset_memory_hooks_state">
<span class="sig-name descname"><span class="pre">reset_memory_hooks_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.reset_memory_hooks_state" title="Permalink to this definition"></a></dt>
<dd><p>Reset the <cite>mem_rss_diff</cite> attribute of each module (see [<cite>~modeling_utils.ModuleUtilsMixin.add_memory_hooks</cite>]).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.resize_position_embeddings">
<span class="sig-name descname"><span class="pre">resize_position_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_num_position_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.resize_position_embeddings" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.resize_token_embeddings">
<span class="sig-name descname"><span class="pre">resize_token_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_num_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.sparse.Embedding</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.resize_token_embeddings" title="Permalink to this definition"></a></dt>
<dd><p>Resizes input token embeddings matrix of the model if <cite>new_num_tokens != config.vocab_size</cite>.</p>
<p>Takes care of tying weights embeddings afterwards if the model class has a <cite>tie_weights()</cite> method.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>new_num_tokens (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The number of new tokens in the embedding matrix. Increasing the size will add newly initialized
vectors at the end. Reducing the size will remove vectors from the end. If not provided or <cite>None</cite>, just
returns a pointer to the input tokens <cite>torch.nn.Embedding</cite> module of the model without doing anything.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p><cite>torch.nn.Embedding</cite>: Pointer to the input tokens Embeddings Module of the model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.retrieve_modules_from_names">
<span class="sig-name descname"><span class="pre">retrieve_modules_from_names</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.retrieve_modules_from_names" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_processor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stopping_criteria</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_stopping_criteria.StoppingCriteriaList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logits_warper</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_logits_process.LogitsProcessorList</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_in_generate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">synced_gpus</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">model_kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.generation_utils.SampleEncoderDecoderOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.generation_utils.SampleDecoderOnlyOutput</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.sample" title="Permalink to this definition"></a></dt>
<dd><p>Generates sequences of token ids for models with a language modeling head using <strong>multinomial sampling</strong> and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.</p>
<p>Parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>input_ids (<cite>torch.LongTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>):</dt><dd><p>The sequence used as a prompt for the generation.</p>
</dd>
<dt>logits_processor (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsProcessor</cite>]
used to modify the prediction scores of the language modeling head applied at each generation step.</p>
</dd>
<dt>stopping_criteria (<cite>StoppingCriteriaList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>StoppingCriteriaList</cite>]. List of instances of class derived from [<cite>StoppingCriteria</cite>]
used to tell if the generation loop should stop.</p>
</dd>
<dt>logits_warper (<cite>LogitsProcessorList</cite>, <em>optional</em>):</dt><dd><p>An instance of [<cite>LogitsProcessorList</cite>]. List of instances of class derived from [<cite>LogitsWarper</cite>] used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.</p>
</dd>
<dt>max_length (<cite>int</cite>, <em>optional</em>, defaults to 20):</dt><dd><p><strong>DEPRECATED</strong>. Use <cite>logits_processor</cite> or <cite>stopping_criteria</cite> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.</p>
</dd>
<dt>pad_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>padding</em> token.</p>
</dd>
<dt>eos_token_id (<cite>int</cite>, <em>optional</em>):</dt><dd><p>The id of the <em>end-of-sequence</em> token.</p>
</dd>
<dt>output_attentions (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under
returned tensors for more details.</p>
</dd>
<dt>output_hidden_states (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors
for more details.</p>
</dd>
<dt>output_scores (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return the prediction scores. See <cite>scores</cite> under returned tensors for more details.</p>
</dd>
<dt>return_dict_in_generate (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
</dd>
<dt>synced_gpus (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
</dd>
<dt>model_kwargs:</dt><dd><p>Additional model specific kwargs will be forwarded to the <cite>forward</cite> function of the model. If model is
an encoder-decoder model the kwargs should include <cite>encoder_outputs</cite>.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Return:</dt><dd><p>[<cite>~generation_utils.SampleDecoderOnlyOutput</cite>], [<cite>~generation_utils.SampleEncoderDecoderOutput</cite>] or
<cite>torch.LongTensor</cite>: A <cite>torch.LongTensor</cite> containing the generated tokens (default behaviour) or a
[<cite>~generation_utils.SampleDecoderOnlyOutput</cite>] if <cite>model.config.is_encoder_decoder=False</cite> and
<cite>return_dict_in_generate=True</cite> or a [<cite>~generation_utils.SampleEncoderDecoderOutput</cite>] if
<cite>model.config.is_encoder_decoder=True</cite>.</p>
</dd>
</dl>
<p>Examples:</p>
<p><a href="#id45"><span class="problematic" id="id46">``</span></a><a href="#id47"><span class="problematic" id="id48">`</span></a>python
&gt;&gt;&gt; from transformers import (
…     AutoTokenizer,
…     AutoModelForCausalLM,
…     LogitsProcessorList,
…     MinLengthLogitsProcessor,
…     TopKLogitsWarper,
…     TemperatureLogitsWarper,
…     StoppingCriteriaList,
…     MaxLengthCriteria,
… )
&gt;&gt;&gt; import torch</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_prompt</span> <span class="o">=</span> <span class="s2">&quot;Today is a beautiful day, and&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_processor</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">MinLengthLogitsProcessor</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># instantiate logits processors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits_warper</span> <span class="o">=</span> <span class="n">LogitsProcessorList</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="n">TopKLogitsWarper</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
<span class="gp">... </span>        <span class="n">TemperatureLogitsWarper</span><span class="p">(</span><span class="mf">0.7</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stopping_criteria</span> <span class="o">=</span> <span class="n">StoppingCriteriaList</span><span class="p">([</span><span class="n">MaxLengthCriteria</span><span class="p">(</span><span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">logits_processor</span><span class="o">=</span><span class="n">logits_processor</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">logits_warper</span><span class="o">=</span><span class="n">logits_warper</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">stopping_criteria</span><span class="o">=</span><span class="n">stopping_criteria</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Today is a beautiful day, and a wonderful day.\n\nI was lucky enough to meet the&#39;]</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.save_pretrained">
<span class="sig-name descname"><span class="pre">save_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">save_directory:</span> <span class="pre">typing.Union[str,</span> <span class="pre">os.PathLike],</span> <span class="pre">is_main_process:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">state_dict:</span> <span class="pre">typing.Optional[dict]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">save_function:</span> <span class="pre">typing.Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">save&gt;,</span> <span class="pre">push_to_hub:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">max_shard_size:</span> <span class="pre">typing.Union[int,</span> <span class="pre">str]</span> <span class="pre">=</span> <span class="pre">'10GB',</span> <span class="pre">**kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.save_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Save a model and its configuration file to a directory, so that it can be re-loaded using the
<cite>[`~PreTrainedModel.from_pretrained</cite>]` class method.</p>
<dl>
<dt>Arguments:</dt><dd><dl>
<dt>save_directory (<cite>str</cite> or <cite>os.PathLike</cite>):</dt><dd><p>Directory to which to save. Will be created if it doesn’t exist.</p>
</dd>
<dt>is_main_process (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>):</dt><dd><p>Whether the process calling this is the main process or not. Useful when in distributed training like
TPUs and need to call this function on all processes. In this case, set <cite>is_main_process=True</cite> only on
the main process to avoid race conditions.</p>
</dd>
<dt>state_dict (nested dictionary of <cite>torch.Tensor</cite>):</dt><dd><p>The state dictionary of the model to save. Will default to <cite>self.state_dict()</cite>, but can be used to only
save parts of the model or if special precautions need to be taken when recovering the state dictionary
of a model (like when using model parallelism).</p>
</dd>
<dt>save_function (<cite>Callable</cite>):</dt><dd><p>The function to use to save the state dictionary. Useful on distributed training like TPUs when one
need to replace <cite>torch.save</cite> by another method.</p>
</dd>
<dt>push_to_hub (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to push your model to the Hugging Face model hub after saving it.</p>
<p>&lt;Tip warning={true}&gt;</p>
<p>Using <cite>push_to_hub=True</cite> will synchronize the repository you are pushing to with <cite>save_directory</cite>,
which requires <cite>save_directory</cite> to be a local clone of the repo you are pushing to if it’s an existing
folder. Pass along <cite>temp_dir=True</cite> to use a temporary directory instead.</p>
<p>&lt;/Tip&gt;</p>
</dd>
<dt>max_shard_size (<cite>int</cite> or <cite>str</cite>, <em>optional</em>, defaults to <cite>“10GB”</cite>):</dt><dd><p>The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size
lower than this size. If expressed as a string, needs to be digits followed by a unit (like <cite>“5MB”</cite>).</p>
<p>&lt;Tip warning={true}&gt;</p>
<p>If a single weight of the model is bigger than <cite>max_shard_size</cite>, it will be in its own checkpoint shard
which will be bigger than <cite>max_shard_size</cite>.</p>
<p>&lt;/Tip&gt;</p>
</dd>
<dt>kwargs:</dt><dd><p>Additional key word arguments passed along to the [<cite>~utils.PushToHubMixin.push_to_hub</cite>] method.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.set_extra_state" title="Permalink to this definition"></a></dt>
<dd><p>This function is called from <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.load_state_dict" title="lambeq.bobcat.BertForChartClassification.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.get_extra_state" title="lambeq.bobcat.BertForChartClassification.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>state (dict): Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.set_input_embeddings">
<span class="sig-name descname"><span class="pre">set_input_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.set_input_embeddings" title="Permalink to this definition"></a></dt>
<dd><p>Set model’s input embeddings.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>value (<cite>nn.Module</cite>): A module mapping vocabulary to hidden states.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.share_memory" title="Permalink to this definition"></a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.state_dict" title="Permalink to this definition"></a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> also accepts positional arguments for
<code class="docutils literal notranslate"><span class="pre">destination</span></code>, <code class="docutils literal notranslate"><span class="pre">prefix</span></code> and <code class="docutils literal notranslate"><span class="pre">keep_vars</span></code> in order. However,
this is being deprecated and keyword arguments will be enforced in
future releases.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please avoid the use of argument <code class="docutils literal notranslate"><span class="pre">destination</span></code> as it is not
designed for end-users.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>destination (dict, optional): If provided, the state of module will</dt><dd><p>be updated into the dict and the same object is returned.
Otherwise, an <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code> will be created and returned.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
<dt>prefix (str, optional): a prefix added to parameter and buffer</dt><dd><p>names to compose the keys in state_dict. Default: <code class="docutils literal notranslate"><span class="pre">''</span></code>.</p>
</dd>
<dt>keep_vars (bool, optional): by default the <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s</dt><dd><p>returned in the state dict are detached from autograd. If it’s
set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, detaching will not be performed.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>dict:</dt><dd><p>a dictionary containing a whole state of the module</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.supports_gradient_checkpointing">
<span class="sig-name descname"><span class="pre">supports_gradient_checkpointing</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.supports_gradient_checkpointing" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.tie_weights">
<span class="sig-name descname"><span class="pre">tie_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.tie_weights" title="Permalink to this definition"></a></dt>
<dd><p>Tie the weights between the input embeddings and the output embeddings.</p>
<p>If the <cite>torchscript</cite> flag is set in the configuration, can’t handle parameter sharing so we are cloning the
weights instead.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.to" title="Permalink to this definition"></a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.channels_last</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point or complex <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.dtype" title="lambeq.bobcat.BertForChartClassification.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>s. In addition, this method will
only cast the floating point or complex parameters and buffers to <a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.dtype" title="lambeq.bobcat.BertForChartClassification.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#lambeq.bobcat.BertForChartClassification.device" title="lambeq.bobcat.BertForChartClassification.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>): the desired device of the parameters</dt><dd><p>and buffers in this module</p>
</dd>
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>): the desired floating point or complex dtype of</dt><dd><p>the parameters and buffers in this module</p>
</dd>
<dt>tensor (torch.Tensor): Tensor whose dtype and device are the desired</dt><dd><p>dtype and device for all parameters and buffers in this module</p>
</dd>
<dt>memory_format (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code>): the desired memory</dt><dd><p>format for 4D parameters and buffers in this module (keyword
only argument)</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.3741+0.j,  0.2382+0.j],</span>
<span class="go">        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">))</span>
<span class="go">tensor([[0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j],</span>
<span class="go">        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.to_empty" title="Permalink to this definition"></a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>): The desired device of the parameters</dt><dd><p>and buffers in this module.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.train" title="Permalink to this definition"></a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>mode (bool): whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation</dt><dd><p>mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.training" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.dtype</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.type" title="Permalink to this definition"></a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><p>dst_type (type or string): the desired type</p>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.T</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.xpu" title="Permalink to this definition"></a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>device (int, optional): if specified, all parameters will be</dt><dd><p>copied to that device</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Module: self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.BertForChartClassification.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.BertForChartClassification.zero_grad" title="Permalink to this definition"></a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>set_to_none (bool): instead of setting to zero, set the grads to None.</dt><dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.Category">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">Category</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">lambeq.bobcat.lexicon.Atom</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">Atom.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">lambeq.bobcat.lexicon.Feature</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">Feature.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">lambeq.bobcat.lexicon.Relation</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'\x00'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">argument</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_raising_dep_var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/lexicon.html#Category"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Category" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The type of a constituent in a CCG.</p>
<p>A category may be atomic (e.g. N) or complex (e.g. S/NP).</p>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">lambeq.bobcat.lexicon.Atom</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">Atom.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">lambeq.bobcat.lexicon.Feature</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">Feature.NONE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">lambeq.bobcat.lexicon.Relation</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'\x00'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">argument</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_raising_dep_var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.Category.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.argument">
<span class="sig-name descname"><span class="pre">argument</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#lambeq.bobcat.Category.argument" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.atom">
<span class="sig-name descname"><span class="pre">atom</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">lambeq.bobcat.lexicon.Atom</span></em><a class="headerlink" href="#lambeq.bobcat.Category.atom" title="Permalink to this definition"></a></dt>
<dd><p>The possible atomic types for a category.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.bwd">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bwd</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.Category.bwd" title="Permalink to this definition"></a></dt>
<dd><p>Whether this is a backward complex category.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.dir">
<span class="sig-name descname"><span class="pre">dir</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'\x00'</span></em><a class="headerlink" href="#lambeq.bobcat.Category.dir" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.feature">
<span class="sig-name descname"><span class="pre">feature</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">lambeq.bobcat.lexicon.Feature</span></em><a class="headerlink" href="#lambeq.bobcat.Category.feature" title="Permalink to this definition"></a></dt>
<dd><p>The possible features for a category.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.fwd">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fwd</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.Category.fwd" title="Permalink to this definition"></a></dt>
<dd><p>Whether this is a forward complex category.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.matches">
<span class="sig-name descname"><span class="pre">matches</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/lexicon.html#Category.matches"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Category.matches" title="Permalink to this definition"></a></dt>
<dd><p>Check if the template set out in this matches the argument.</p>
<p>Like == but the NONE feature matches with everything.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.parse">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">string</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_raising_dep_var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'+'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a></span></span><a class="reference internal" href="_modules/lambeq/bobcat/lexicon.html#Category.parse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Category.parse" title="Permalink to this definition"></a></dt>
<dd><p>Parse a category string.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.relation">
<span class="sig-name descname"><span class="pre">relation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">lambeq.bobcat.lexicon.Relation</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#lambeq.bobcat.Category.relation" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.result">
<span class="sig-name descname"><span class="pre">result</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#lambeq.bobcat.Category.result" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.slash">
<span class="sig-name descname"><span class="pre">slash</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">argument</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">lambeq.bobcat.lexicon.Relation</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_raising_dep_var</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.lexicon.Category"><span class="pre">lambeq.bobcat.lexicon.Category</span></a></span></span><a class="reference internal" href="_modules/lambeq/bobcat/lexicon.html#Category.slash"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Category.slash" title="Permalink to this definition"></a></dt>
<dd><p>Create a complex category.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.translate">
<span class="sig-name descname"><span class="pre">translate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">var_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Feature</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">Feature.NONE</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.Category"><span class="pre">Category</span></a></span></span><a class="reference internal" href="_modules/lambeq/bobcat/lexicon.html#Category.translate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Category.translate" title="Permalink to this definition"></a></dt>
<dd><p>Translate a category.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>var_map</strong><span class="classifier">dict of int to int</span></dt><dd><p>A mapping to relabel variable slots.</p>
</dd>
<dt><strong>feature</strong><span class="classifier">Feature, optional</span></dt><dd><p>The concrete feature for variable features.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.type_raising_dep_var">
<span class="sig-name descname"><span class="pre">type_raising_dep_var</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#lambeq.bobcat.Category.type_raising_dep_var" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Category.var">
<span class="sig-name descname"><span class="pre">var</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#lambeq.bobcat.Category.var" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">ChartParser</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grammar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.Grammar" title="lambeq.bobcat.Grammar"><span class="pre">Grammar</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">cats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root_cats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eisner_normal_form</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_parse_trees</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tag_score_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">missing_cat_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">missing_span_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sentence</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.Sentence" title="lambeq.bobcat.parser.Sentence"><span class="pre">lambeq.bobcat.parser.Sentence</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">lambeq.bobcat.parser.ParseResult</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Parse a sentence.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grammar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.Grammar" title="lambeq.bobcat.Grammar"><span class="pre">Grammar</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">cats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root_cats</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eisner_normal_form</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_parse_trees</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tag_score_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">missing_cat_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">missing_span_score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser.calc_score_binary">
<span class="sig-name descname"><span class="pre">calc_score_binary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tree</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser.calc_score_binary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser.calc_score_binary" title="Permalink to this definition"></a></dt>
<dd><p>Calculate the score for a binary tree.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser.calc_score_unary">
<span class="sig-name descname"><span class="pre">calc_score_unary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tree</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser.calc_score_unary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser.calc_score_unary" title="Permalink to this definition"></a></dt>
<dd><p>Calculate the score for a unary tree (chain).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser.filter_root">
<span class="sig-name descname"><span class="pre">filter_root</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trees</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser.filter_root"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser.filter_root" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ChartParser.get_span_score">
<span class="sig-name descname"><span class="pre">get_span_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">span_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#ChartParser.get_span_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ChartParser.get_span_score" title="Permalink to this definition"></a></dt>
<dd><p>Get the score in a span for a category (chain) ID.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">Grammar</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">categories</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary_rules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_changing_rules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_raising_rules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/grammar.html#Grammar"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Grammar" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The grammar dataclass.</p>
<dl class="field-list">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl>
<dt><strong>categories</strong><span class="classifier">dict of str to str</span></dt><dd><p>A mapping from a plain category string to a marked up category
string, e.g. ‘(NPNP)/NP’ to ‘((NP{Y}NP{Y}&lt;1&gt;){_}/NP{Z}&lt;2&gt;){_}’</p>
</dd>
<dt><strong>binary_rules: list of tuple of str</strong></dt><dd><p>The list of binary rules as tuple pairs of strings,
e.g. (‘(N/N)’, ‘N’)</p>
</dd>
<dt><strong>type_changing_rules</strong><span class="classifier">list of tuple</span></dt><dd><p>The list of type changing rules, which may occur as either unary
rules or punctuation rules, as tuples of:</p>
<blockquote>
<div><ul class="simple">
<li><p>an integer denoting the rule ID</p></li>
<li><p>a string denoting the left category, or the sole if unary</p></li>
<li><p>a string denoting the right category, or None if unary</p></li>
<li><p>a string denoting the resulting category</p></li>
<li><p>a boolean denoting whether to replace dependencies
during parsing</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>e.g. (1, ‘N’, None, ‘NP’, False)</dt><dd><p>(50, ‘S[dcl]/S[dcl]’, ‘,’, ‘S/S’, True)</p>
</dd>
</dl>
</dd>
<dt><strong>type_raising_rules</strong><span class="classifier">list of tuple</span></dt><dd><dl class="simple">
<dt>The list of type raising rules as tuples of:</dt><dd><ul class="simple">
<li><p>a string denoting the original category</p></li>
<li><p>a string denoting the resulting marked-up category</p></li>
<li><p>a character denoting the new variable</p></li>
</ul>
</dd>
</dl>
<p>e.g. (‘NP’, ‘(S[X]{Y}/(S[X]{Y}NP{_}){Y}){Y}’, ‘+’)</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">categories</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary_rules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_changing_rules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_raising_rules</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.Grammar.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.binary_rules">
<span class="sig-name descname"><span class="pre">binary_rules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Grammar.binary_rules" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.categories">
<span class="sig-name descname"><span class="pre">categories</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Grammar.categories" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.load">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">os.PathLike</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#lambeq.bobcat.Grammar" title="lambeq.bobcat.grammar.Grammar"><span class="pre">lambeq.bobcat.grammar.Grammar</span></a></span></span><a class="reference internal" href="_modules/lambeq/bobcat/grammar.html#Grammar.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Grammar.load" title="Permalink to this definition"></a></dt>
<dd><p>Load a grammar from a JSON file.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">os.PathLike</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/grammar.html#Grammar.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Grammar.save" title="Permalink to this definition"></a></dt>
<dd><p>Save the grammar to a JSON file.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.type_changing_rules">
<span class="sig-name descname"><span class="pre">type_changing_rules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Grammar.type_changing_rules" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Grammar.type_raising_rules">
<span class="sig-name descname"><span class="pre">type_raising_rules</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Grammar.type_raising_rules" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">ParseTree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rule</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'Rule'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'Category'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">left</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'ParseTree'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">right</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'ParseTree'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unfilled_deps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'list[Dependency]'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filled_deps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'list[Dependency]'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'dict[int,</span> <span class="pre">Variable]'</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="s"><span class="pre">'float'</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/tree.html#ParseTree"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.ParseTree" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rule</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Rule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.Category"><span class="pre">Category</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">left</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">right</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">unfilled_deps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Dependency</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filled_deps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Dependency</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_map</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Variable</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.ParseTree.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.bwd_comp">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bwd_comp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.bwd_comp" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.cat">
<span class="sig-name descname"><span class="pre">cat</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#lambeq.bobcat.Category" title="lambeq.bobcat.Category"><span class="pre">Category</span></a></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.cat" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.coordinated">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">coordinated</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.coordinated" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.coordinated_or_type_raised">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">coordinated_or_type_raised</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.coordinated_or_type_raised" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.filled_deps">
<span class="sig-name descname"><span class="pre">filled_deps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Dependency</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.filled_deps" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.fwd_comp">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fwd_comp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.fwd_comp" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.left">
<span class="sig-name descname"><span class="pre">left</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.left" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.right">
<span class="sig-name descname"><span class="pre">right</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#lambeq.bobcat.ParseTree" title="lambeq.bobcat.ParseTree"><span class="pre">ParseTree</span></a></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.right" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.rule">
<span class="sig-name descname"><span class="pre">rule</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Rule</span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.rule" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.score">
<span class="sig-name descname"><span class="pre">score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.score" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.unfilled_deps">
<span class="sig-name descname"><span class="pre">unfilled_deps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Dependency</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.unfilled_deps" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.var_map">
<span class="sig-name descname"><span class="pre">var_map</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Variable</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.var_map" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lambeq.bobcat.ParseTree.word">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">word</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.ParseTree.word" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.Sentence">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">Sentence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_supertags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Supertag" title="lambeq.bobcat.Supertag"><span class="pre">Supertag</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">SpanT</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#Sentence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Sentence" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An input sentence.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>words</strong><span class="classifier">list of str</span></dt><dd><p>The tokens in the sentence.</p>
</dd>
<dt><strong>input_supertags</strong><span class="classifier">list of list of Supertag</span></dt><dd><p>A list of supertags for each word.</p>
</dd>
<dt><strong>span_scores</strong><span class="classifier">dict of tuple of int and int to dict of int to float</span></dt><dd><p>Mapping of a span to a dict of category (indices) mapped to
their log probability.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Sentence.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_supertags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Supertag" title="lambeq.bobcat.Supertag"><span class="pre">Supertag</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">SpanT</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.Sentence.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Sentence.input_supertags">
<span class="sig-name descname"><span class="pre">input_supertags</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#lambeq.bobcat.Supertag" title="lambeq.bobcat.Supertag"><span class="pre">Supertag</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Sentence.input_supertags" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Sentence.span_scores">
<span class="sig-name descname"><span class="pre">span_scores</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">SpanT</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Sentence.span_scores" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Sentence.words">
<span class="sig-name descname"><span class="pre">words</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#lambeq.bobcat.Sentence.words" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.Supertag">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">Supertag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">category</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/parser.html#Supertag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Supertag" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A string category, annotated with its log probability.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Supertag.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">category</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#lambeq.bobcat.Supertag.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Supertag.category">
<span class="sig-name descname"><span class="pre">category</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><a class="headerlink" href="#lambeq.bobcat.Supertag.category" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lambeq.bobcat.Supertag.probability">
<span class="sig-name descname"><span class="pre">probability</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#lambeq.bobcat.Supertag.probability" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lambeq.bobcat.Tagger">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lambeq.bobcat.</span></span><span class="sig-name descname"><span class="pre">Tagger</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.modeling_utils.PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.tokenization_utils_fast.PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_prob_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_prob_threshold_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relative'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_prob_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_prob_threshold_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relative'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#Tagger"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Tagger" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Tagger.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'progress'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">lambeq.bobcat.tagger.TaggerOutput</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#Tagger.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Tagger.__call__" title="Permalink to this definition"></a></dt>
<dd><p>Parse a list of sentences.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Tagger.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.modeling_utils.PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.tokenization_utils_fast.PreTrainedTokenizerFast</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_prob_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag_prob_threshold_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relative'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_top_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_prob_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">span_prob_threshold_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relative'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#Tagger.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Tagger.__init__" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Tagger.parse">
<span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">TaggerOutputSentence</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#Tagger.parse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Tagger.parse" title="Permalink to this definition"></a></dt>
<dd><p>Parse a batch of sentences.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lambeq.bobcat.Tagger.prepare_inputs">
<span class="sig-name descname"><span class="pre">prepare_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/lambeq/bobcat/tagger.html#Tagger.prepare_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lambeq.bobcat.Tagger.prepare_inputs" title="Permalink to this definition"></a></dt>
<dd><p>Prepare a batch of sentences for parsing.</p>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, 2022, Cambridge Quantum Computing Ltd..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>