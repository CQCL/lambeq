<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Text classification &mdash; lambeq 0.4.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/table-wrap.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Machine learning best practices" href="nlp-ml.html" />
    <link rel="prev" title="Working with text data" href="nlp-data.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            lambeq
              <img src="_static/lambeq_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.4.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="parsing.html">Syntactic parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="string-diagrams.html">String diagrams</a></li>
<li class="toctree-l1"><a class="reference internal" href="use-cases.html">lambeq use cases</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributing to lambeq</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NLP-101</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nlp-intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-data.html">Working with text data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Text classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#binary-vs-multi-class-classification">Binary vs multi-class classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss-functions">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-metrics">Evaluation metrics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nlp-ml.html">Machine learning best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp-refs.html">References for further study</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/sentence-input.html">Step 1. Sentence input</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/rewrite.html">Step 2. Diagram rewriting</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/parameterise.html">Step 3. Parameterisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Step 4: Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Choosing a model</a></li>
<li class="toctree-l1"><a class="reference internal" href="manual-training.html">Advanced: Manual training</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced.html">Advanced: low-level lambeq</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/extend-lambeq.html">Advanced: Extending lambeq</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Toolkit</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="root-api.html">lambeq package</a></li>
<li class="toctree-l1"><a class="reference internal" href="package-api.html">Subpackages</a></li>
<li class="toctree-l1"><a class="reference internal" href="uml-diagrams.html">Class diagrams</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command-line interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="genindex.html">Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">External links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://qnlp.cambridgequantum.com/downloads.html">Resources</a></li>
<li class="toctree-l1"><a class="reference external" href="https://qnlp.cambridgequantum.com/generate.html">Web demo</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discopy.readthedocs.io">DisCoPy</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">lambeq</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Text classification</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/CQCL/lambeq/blob/main/docs/nlp-class.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="text-classification">
<h1>Text classification<a class="headerlink" href="#text-classification" title="Permalink to this heading"></a></h1>
<p>One of the most fundamental tasks in NLP is text classification, which involves categorising textual data into predefined categories. It plays a vital role in a variety of NLP applications, including sentiment analysis, spam detection, topic modeling, and language identification, among others. By categorising texts into relevant categories, machines can analyse and derive insights from large volumes of textual data, making it possible to automate decision-making processes and perform tasks that would otherwise be time-consuming or impossible for humans to do.</p>
<section id="binary-vs-multi-class-classification">
<h2>Binary vs multi-class classification<a class="headerlink" href="#binary-vs-multi-class-classification" title="Permalink to this heading"></a></h2>
<p>Binary classification and multi-class classification involve assigning a label or category to an input data point. In <cite>binary classification</cite>, there are only two possible output categories, and the goal is to classify input data points into one of these two categories. For example, classifying emails as spam or not spam.</p>
<p>On the other hand, <cite>multi-class classification</cite> involves assigning a data point to one of more than two possible output categories. For example, classifying images of animals into categories such as cats, dogs, and birds.</p>
<p>Multi-class classification problems can be further divided into two subcategories: multi-class <cite>single-label</cite> classification and multi-class <cite>multi-label</cite> classification. In multi-class single-label classification, each input data point is assigned to one and only one output category. In contrast, in multi-class multi-label classification, each input data point can be assigned to one or more output categories simultaneously.</p>
<p>In general, binary classification is a simpler and more straightforward problem to solve than multi-class classification, but multi-class classification problems are more representative of real-world scenarios where there are multiple possible categories to that a data point could belong.</p>
</section>
<section id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this heading"></a></h2>
<p>For binary classification tasks, the loss function of choice is binary cross-entropy. Below, <span class="math notranslate nohighlight">\(y_i\)</span> is the true label for the <span class="math notranslate nohighlight">\(i\)</span> th data point, <span class="math notranslate nohighlight">\(p(y_i)\)</span> represents the probability that the model assigns to the specific label, and <span class="math notranslate nohighlight">\(N\)</span> is the number of data points.</p>
<div class="math notranslate nohighlight">
\[H(p, q) = -\frac{1}{N}\sum_{i=1}^N [y_i \log(p(y_i)) + (1-y_i) \log(1-p(y_i))]\]</div>
<p>For multi-class classification, the loss function is usually the categorical version of cross-entropy. Here, <span class="math notranslate nohighlight">\(M\)</span> is the number of classes, <span class="math notranslate nohighlight">\(p(x_i)\)</span> is the true probability for the <span class="math notranslate nohighlight">\(i\)</span> th class, and <span class="math notranslate nohighlight">\(q(x_i)\)</span> the probability predicted by the model.</p>
<div class="math notranslate nohighlight">
\[H(p, q) = -\sum_{i=1}^M p(x_i) \log(q(x_i))\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">lambeq</span></code> provides a number of loss functions that can be used out-of-the-box during training, such as <a class="reference internal" href="lambeq.training.html#lambeq.training.BinaryCrossEntropyLoss" title="lambeq.training.BinaryCrossEntropyLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code></a>, <a class="reference internal" href="lambeq.training.html#lambeq.training.CrossEntropyLoss" title="lambeq.training.CrossEntropyLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a>, and <a class="reference internal" href="lambeq.training.html#lambeq.training.MSELoss" title="lambeq.training.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a>.</p>
</div>
</section>
<section id="evaluation-metrics">
<span id="sec-evaluation"></span><h2>Evaluation metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this heading"></a></h2>
<p>The most common metrics to evaluate the performance of classification models is accuracy, precision, recall, and F-score. Each metric has its own strengths and weaknesses, and can be useful in different contexts.</p>
<ul class="simple">
<li><p><cite>Accuracy</cite> is usually the standard way to evaluate classification, and it measures how often the model correctly predicts the class of an instance. It is calculated as the ratio of correct predictions to the total number of predictions. This metric can be useful when the classes in the dataset are balanced, meaning that there are roughly equal numbers of instances in each class. In this case, accuracy can provide a good overall measure of how well the model is performing.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{True Positives} + \text{True Negatives} + \text{False Positives} + \text{False Negatives}}\]</div>
<ul class="simple">
<li><p><cite>Precision</cite> is the proportion of true positive predictions among all positive predictions. It is expressed as the ratio of true positives to the total number of instances that the model predicts as positive. Precision is useful when the cost of false positives is high, such as in spam filtering or legal decision making.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}\]</div>
<ul class="simple">
<li><p><cite>Recall</cite>, also known as <cite>sensitivity</cite>, is the proportion of true positive predictions among all actual positive instances in the dataset. Recall is calculated as the ratio of true positives to the total number of instances of that class. It can be helpful when the goal of the model is to identify all instances of a particular class, such as in medical diagnosis or fraud detection.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}\]</div>
<p>These two measures can be competing in the sense that increasing precision can decrease recall and vice versa. This trade-off occurs because precision and recall measure different aspects of the model’s performance. High precision means that the model is accurate in its positive predictions, but it may miss some true positive instances, leading to lower recall. On the other hand, high recall means that the model identifies most of the positive instances, but it may have more false positives, leading to lower precision.</p>
<p>To address this, researchers use <cite>F-score</cite>, also known as the <cite>F1</cite> score, which is a combined measure of precision and recall. It is calculated as the harmonic mean of precision and recall and provides a way to balance these two metrics. F-score is useful when both precision and recall are important and can be used to compare models that have different tradeoffs between these two metrics.</p>
<div class="math notranslate nohighlight">
\[\text{F-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For examples of text classification with <code class="docutils literal notranslate"><span class="pre">lambeq</span></code>, see the <a class="reference internal" href="training.html#sec-training"><span class="std std-ref">Training tutorial</span></a>.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nlp-data.html" class="btn btn-neutral float-left" title="Working with text data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="nlp-ml.html" class="btn btn-neutral float-right" title="Machine learning best practices" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2024 Cambridge Quantum Computing Ltd..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>